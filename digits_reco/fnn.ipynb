{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 784) (42000, 10)\n",
      "    Test set (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "dummies = pd.get_dummies(train_data['label'], prefix='label', drop_first=False)\n",
    "train_data = train_data\n",
    "train_data = train_data.drop('label', axis=1)\n",
    "labels = dummies\n",
    "\n",
    "#print(labels)\n",
    "\n",
    "train_samples, train_labels =train_data, labels\n",
    "test_samples = test_data\n",
    "\n",
    "print('Training set', train_samples.shape, train_labels.shape)\n",
    "print('    Test set', test_samples.shape)\n",
    "\n",
    "image_size = 28\n",
    "num_channels = 1\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_chunk(samples, labels, chunkSize):\n",
    "    '''\n",
    "    Iterator/Generator: get a batch of data\n",
    "    这个函数是一个迭代器/生成器，用于每一次只得到 chunkSize 这么多的数据\n",
    "    用于 for loop， just like range() function\n",
    "    '''\n",
    "#     if len(samples) != len(labels):\n",
    "#         raise Exception('Length of samples and labels must equal')\n",
    "#     stepStart = 0\t# initial step\n",
    "#     i = 0\n",
    "#     while stepStart < len(samples):\n",
    "#         stepEnd = stepStart + chunkSize\n",
    "#         if stepEnd < len(samples):\n",
    "#             yield i, samples[stepStart:stepEnd], labels[stepStart:stepEnd]\n",
    "#         i += 1\n",
    "#         stepStart = stepEnd\n",
    "    datas = pd.concat([labels, samples], axis=1)\n",
    "    random_batch = datas.loc[np.random.randint(42000, size=100)]\n",
    "    images = random_batch.iloc[:, 10:].values\n",
    "    images = images.astype(np.float)\n",
    "    # convert from [0:255] => [0.0:1.0]\n",
    "    images = np.multiply(images, 1.0 / 255.0)\n",
    "        \n",
    "    labels = random_batch.iloc[:, 0:10].values\n",
    "    return images, labels\n",
    "    \n",
    "\n",
    "class Network():\n",
    "    def __init__(self, num_hidden, batch_size):\n",
    "        '''\n",
    "        @num_hidden: 隐藏层的节点数量\n",
    "        ＠batch_size: 节省内存 所以分批处理数据 每一批的数据量\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = 28000\n",
    "        \n",
    "        # hyper parameters (超参数)\n",
    "        self.num_hidden = num_hidden\n",
    "        \n",
    "        # Graph Related (计算图谱相关)\n",
    "        self.graph = tf.Graph()\n",
    "        self.tf_train_samples = None\n",
    "        self.tf_train_labels = None\n",
    "        self.tf_test_samples = None\n",
    "        self.tf_test_labels = None\n",
    "        self.tf_test_prediction = None\n",
    "        \n",
    "        \n",
    "    def define_graph(self):\n",
    "        '''\n",
    "        定义计算图谱\n",
    "        '''\n",
    "        with self.graph.as_default():\n",
    "            # 这里只是定义图谱中的各种变量\n",
    "            self.tf_train_samples = tf.placeholder(\n",
    "                tf.float32, shape=(self.batch_size, 784)\n",
    "            )\n",
    "            self.tf_train_labels  = tf.placeholder(\n",
    "                tf.float32, shape=(self.batch_size, num_labels)\n",
    "            )\n",
    "            self.tf_test_samples  = tf.placeholder(\n",
    "                tf.float32, shape=(self.test_batch_size, 784)\n",
    "            )\n",
    "            \n",
    "            # 全连接的第一层(layer 1)，注意这里并没有权重共享\n",
    "            fc1_weights = tf.Variable(\n",
    "                tf.truncated_normal([image_size * image_size, self.num_hidden], stddev=0.1)\n",
    "            )\n",
    "            # fc1_biases = tf.Variable(tf.zeros([self.num_hidden]))\n",
    "            fc1_biases = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]))\n",
    "            \n",
    "            # 全连接的第二层(layer 2)\n",
    "            fc2_weights = tf.Variable(\n",
    "                tf.truncated_normal([self.num_hidden, num_labels], stddev=0.1)\n",
    "            )\n",
    "            fc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n",
    "            \n",
    "            # 定义图谱的运算\n",
    "            def model(data):\n",
    "                # fc1\n",
    "                #shape = data.get_shape().as_list()\n",
    "                #print(data.get_shape(), shape)\n",
    "                #reshape\n",
    "                #reshape_data = tf.reshape(data, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                hidden = tf.nn.relu(tf.matmul(data, fc1_weights) + fc1_biases)\n",
    "                \n",
    "                # fc2\n",
    "                return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "            \n",
    "            # Training computation. (训练)\n",
    "            logits = model(self.tf_train_samples)\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.tf_train_labels)\n",
    "            )\n",
    "\n",
    "            # Optimizer. (优化器)\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(self.loss)\n",
    "\n",
    "            # Predictions for the training, validation, and test data. (预测数据)\n",
    "            self.train_prediction = tf.nn.softmax(logits)\n",
    "            self.test_prediction = tf.nn.softmax(model(self.tf_test_samples))\n",
    "            \n",
    "        \n",
    "    def run(self):\n",
    "        '''\n",
    "        用到Session\n",
    "        '''\n",
    "        # 训练\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        with self.session as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('开始训练')\n",
    "            # batch 1000\n",
    "            for i in range(1000):\n",
    "                samples, labels = get_chunk(train_samples, train_labels, chunkSize=self.batch_size)\n",
    "                _, l, predictions = session.run(\n",
    "                    [self.optimizer, self.loss, self.train_prediction],\n",
    "                    feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels}\n",
    "                )\n",
    "                #print(labels.shape)\n",
    "                accuracy, _ = self.accuracy(predictions, labels)\n",
    "                if i % 50 == 0:\n",
    "                    print('Minibatch loss as step %d: %f' % (i, l))\n",
    "                    print('Minibatch accuracy: %.lf%%' % accuracy)\n",
    "            #test_predictions = session.run([self.test_prediction], feed_dict={self.tf_test_samples: test_samples})\n",
    "            #print(\"test shape is {}\".format(test_predictions))\n",
    "    \n",
    "    def accuracy(self, predictions, labels, need_confusion_matrix=False):\n",
    "        '''\n",
    "        计算预测的正确率与召回率\n",
    "        @return: accuracy and confusionMatrix as a tuple\n",
    "        '''\n",
    "        #print(type(predictions.shape))\n",
    "        _predictions = np.argmax(predictions, 1)\n",
    "        #print(_predictions)\n",
    "        #print(labels.shape)\n",
    "        #labels = [tuple(x) for x in labels.to_records(index=False)]\n",
    "        labels = tuple(map(tuple, labels))\n",
    "        _labels = np.argmax(labels, 1)\n",
    "        #print(_labels.shape)\n",
    "        cm = confusion_matrix(_labels, _predictions) if need_confusion_matrix else None\n",
    "        # == is overloaded for numpy array\n",
    "        accuracy = (100.0 * np.sum(_predictions == _labels) / predictions.shape[0])\n",
    "        return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Network'>\n",
      "开始训练\n",
      "Minibatch loss as step 0: 2.726348\n",
      "Minibatch accuracy: 1%\n",
      "Minibatch loss as step 50: 1.840623\n",
      "Minibatch accuracy: 45%\n",
      "Minibatch loss as step 100: 1.340141\n",
      "Minibatch accuracy: 69%\n",
      "Minibatch loss as step 150: 1.280504\n",
      "Minibatch accuracy: 69%\n",
      "Minibatch loss as step 200: 0.866129\n",
      "Minibatch accuracy: 83%\n",
      "Minibatch loss as step 250: 0.825664\n",
      "Minibatch accuracy: 79%\n",
      "Minibatch loss as step 300: 0.743143\n",
      "Minibatch accuracy: 84%\n",
      "Minibatch loss as step 350: 0.632484\n",
      "Minibatch accuracy: 84%\n",
      "Minibatch loss as step 400: 0.681659\n",
      "Minibatch accuracy: 82%\n",
      "Minibatch loss as step 450: 0.732495\n",
      "Minibatch accuracy: 77%\n",
      "Minibatch loss as step 500: 0.670477\n",
      "Minibatch accuracy: 85%\n",
      "Minibatch loss as step 550: 0.639655\n",
      "Minibatch accuracy: 82%\n",
      "Minibatch loss as step 600: 0.563874\n",
      "Minibatch accuracy: 85%\n",
      "Minibatch loss as step 650: 0.536616\n",
      "Minibatch accuracy: 85%\n",
      "Minibatch loss as step 700: 0.490733\n",
      "Minibatch accuracy: 84%\n",
      "Minibatch loss as step 750: 0.659809\n",
      "Minibatch accuracy: 86%\n",
      "Minibatch loss as step 800: 0.445017\n",
      "Minibatch accuracy: 88%\n",
      "Minibatch loss as step 850: 0.355614\n",
      "Minibatch accuracy: 91%\n",
      "Minibatch loss as step 900: 0.477791\n",
      "Minibatch accuracy: 87%\n",
      "Minibatch loss as step 950: 0.416883\n",
      "Minibatch accuracy: 87%\n"
     ]
    }
   ],
   "source": [
    "net = Network(num_hidden = 256, batch_size = 100)\n",
    "print(type(net))\n",
    "net.define_graph()\n",
    "net.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
